{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1cbfeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "val_df = pd.read_pickle(\"val_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c4e74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set for reproducibility.\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)                           # Python random\n",
    "    np.random.seed(seed)                        # NumPy\n",
    "    torch.manual_seed(seed)                     # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)                # PyTorch GPU\n",
    "    torch.backends.cudnn.deterministic = True   \n",
    "    torch.backends.cudnn.benchmark = False      \n",
    "\n",
    "set_seed(42)\n",
    "print(\"Seed set for reproducibility.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba54c5",
   "metadata": {},
   "source": [
    "# Model-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022da6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# STDFT function to compute the Short-Time Fourier Transform (STFT) for a batch of signals\n",
    "# It convert into time -frequency representation\n",
    "# n_fft: size of the FFT window(300 hz is the sampling rate, so 256 is a good choice to capture more than 1 heart beat)\n",
    "# hop_length: number of samples between successive frames (128 is a good choice since every window will overlap by 50%)\n",
    "def compute_stft_batch(x, n_fft=256, hop_length=128):\n",
    "    stft = torch.stft(\n",
    "        x, n_fft=n_fft, hop_length=hop_length,\n",
    "        return_complex=True\n",
    "    )\n",
    "    return torch.abs(stft)  \n",
    "\n",
    "# 2. Model\n",
    "class ECGModel(nn.Module):\n",
    "    def __init__(self,hidden_size=128, dropout_rate=0.0):\n",
    "        super(ECGModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "\n",
    "        # RNN\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=32 * 32, # out_channels * frequency_bins // 2 // 2 (due to max pooling) ()\n",
    "            hidden_size= hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Fully connected\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_size, 4)  # 4 classes: Normal, AF, Other, Noisy\n",
    "\n",
    "    def forward(self, x:torch.Tensor)-> torch.Tensor:\n",
    "        # x: (batch_size, signal_length)\n",
    "        #print(\"Input:\", x.shape)\n",
    "        x = compute_stft_batch(x)  # STFT → (batch, freq, time)\n",
    "        #print(\"After STFT:\", x.shape)\n",
    "        x = torch.log1p(x)  # logarithmic scaling\n",
    "        #print(\"After log1p:\", x.shape)\n",
    "        \n",
    "\n",
    "        x = x.unsqueeze(1)  # CNN input shape: (batch, channel, freq, time)\n",
    "        #print(\"After Unsqueeze:\", x.shape)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        #print(\"After conv2:\", x.shape)\n",
    "\n",
    "        # Flatten the output for RNN input\n",
    "        b, c, f, t = x.shape  # batch, channel, freq, time\n",
    "        x = x.view(b, c * f, t)  # (batch, features, time)\n",
    "        #print(\"After view:\", x.shape)\n",
    "        x = x.permute(0, 2, 1)   # (batch, time, features)\n",
    "        #print(\"After permute:\", x.shape)\n",
    "\n",
    "        # RNN\n",
    "        output, h_n = self.rnn(x)\n",
    "        #print(\"After RNN output:\", output.shape)\n",
    "        x = self.dropout(h_n[-1])\n",
    "        x = self.fc(x)  # use the last hidden state for classification\n",
    "        #print(\"Final output:\", x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c99f8",
   "metadata": {},
   "source": [
    "## Signal Padding and Trimming\n",
    "In our dataset the signal lengths are not fixed, for example:\n",
    "\n",
    "Min: ~2700\n",
    "\n",
    "Max: ~18286\n",
    "\n",
    "But a very large part: 9000\n",
    "\n",
    "Therefore we have to process this length difference when giving the signal to your model. Otherwise, Tensor sizes do not match and we cannot process in batch on GPU\n",
    "\n",
    "1. We will padding very short signals with 0 → complete to 9000\n",
    "\n",
    "2. We will cut very long signals and reduce to 9000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85da27c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_trim(signal, target_length=9000):\n",
    "    current_length = len(signal)\n",
    "\n",
    "    if current_length < target_length:\n",
    "        # Pad with zeros at the end\n",
    "        padding = target_length - current_length\n",
    "        signal = np.pad(signal, (0, padding), 'constant')\n",
    "    elif current_length > target_length:\n",
    "        # Trim from center\n",
    "        start = (current_length - target_length) // 2\n",
    "        signal = signal[start : start + target_length]\n",
    "\n",
    "    return signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfc2502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, df, target_length=9000):\n",
    "        self.df = df\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Signal processing\n",
    "        # Pad or trim the signal to the target length\n",
    "        signal = pad_or_trim(row['signal'], self.target_length)\n",
    "        signal = torch.tensor(signal, dtype=torch.float32)\n",
    "        label = int(row['label']) \n",
    "\n",
    "        return signal, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c6dd4",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "We use PyTorch's DataLoader to handle the batching and shuffling of ECG data.\n",
    "This allows efficient training by automatically grouping samples into mini-batches,\n",
    "converting signals and labels into tensors, and optionally shuffling the training data each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c09a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ECGDataset(train_df)\n",
    "val_dataset = ECGDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,generator=torch.Generator().manual_seed(42))\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b12de10",
   "metadata": {},
   "source": [
    "## Base Model Parameter Choice\n",
    "This is the baseline configuration. We use ECGModel() with default parameters (e.g., hidden size, dropout), and Adam optimizer with a learning rate of 0.001. These choices are standard and can be further tuned later to improve model performance. Also for the error function,we use CrossEntropyLoss because:\n",
    "\n",
    "1. We are doing a multi-class (4-class) task and it is a classic multi-class classification problem.\n",
    "\n",
    "2. It can calculate the loss by taking the outputs (logits) of the model directly. The higher the probability that the model gives to the correct class, the lower the loss.\n",
    "\n",
    "3. We corrected the class imbalance with class_weights in later stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d32c862c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emert\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ECGModel().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4200cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "def train_one_epoch(model:nn.Module, \n",
    "                    dataloader:torch.utils.data.DataLoader, \n",
    "                    optimizer:torch.optim.Optimizer, \n",
    "                    loss_fn:nn.Module):\n",
    "    \"\"\"\n",
    "    Trains the model for one full epoch.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model (ECGModel).\n",
    "        dataloader: The training DataLoader providing batches.\n",
    "        optimizer: The optimizer (e.g., Adam).\n",
    "        loss_fn: The loss function (e.g., CrossEntropyLoss).\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (average_loss, accuracy) for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for signals, labels in dataloader:\n",
    "        signals = signals.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(signals)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * signals.size(0)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2dd6b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model:nn.Module, dataloader:torch.utils.data.DataLoader, loss_fn:nn.Module):\n",
    "    \"\"\"\n",
    "    Evaluates the model on validation data.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model to evaluate.\n",
    "        dataloader: DataLoader for validation data.\n",
    "        loss_fn: Same loss function used during training.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (average_loss, accuracy) for validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for signals, labels in dataloader:\n",
    "            signals = signals.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(signals)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * signals.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70318d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_with_metrics(model:nn.Module, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for signals, labels in dataloader:\n",
    "            signals = signals.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(signals)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = 100 * np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[0, 1, 2, 3])\n",
    "    report = classification_report(\n",
    "    all_labels, all_preds,\n",
    "    labels=[0, 1, 2, 3],\n",
    "    target_names=['Normal', 'AF', 'Other', 'Noisy'],\n",
    "    zero_division=0  # uyarı vermesin\n",
    ")\n",
    "\n",
    "    return avg_loss, acc, f1, cm, report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbebe63",
   "metadata": {},
   "source": [
    "## First Base Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78e70a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.9944 | Train Acc: 0.5889\n",
      "Val   Loss: 1.0063 | Val   Acc: 0.5782\n",
      "----------------------------------------\n",
      "Epoch 2/5\n",
      "Train Loss: 0.9892 | Train Acc: 0.5874\n",
      "Val   Loss: 1.0086 | Val   Acc: 0.5868\n",
      "----------------------------------------\n",
      "Epoch 3/5\n",
      "Train Loss: 0.9754 | Train Acc: 0.5901\n",
      "Val   Loss: 0.9281 | Val   Acc: 0.6052\n",
      "----------------------------------------\n",
      "Epoch 4/5\n",
      "Train Loss: 0.9541 | Train Acc: 0.5971\n",
      "Val   Loss: 0.9905 | Val   Acc: 0.5868\n",
      "----------------------------------------\n",
      "Epoch 5/5\n",
      "Train Loss: 0.9062 | Train Acc: 0.6116\n",
      "Val   Loss: 0.8805 | Val   Acc: 0.6246\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, loss_fn)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "149986a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.3092\n",
      "Confusion Matrix:\n",
      " [[520   0  24   0]\n",
      " [ 57   0  22   5]\n",
      " [207   0  55   2]\n",
      " [ 13   0  18   4]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.65      0.96      0.78       544\n",
      "          AF       0.00      0.00      0.00        84\n",
      "       Other       0.46      0.21      0.29       264\n",
      "       Noisy       0.36      0.11      0.17        35\n",
      "\n",
      "    accuracy                           0.62       927\n",
      "   macro avg       0.37      0.32      0.31       927\n",
      "weighted avg       0.53      0.62      0.54       927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc, val_f1, val_cm, val_report = evaluate_with_metrics(model, val_loader, loss_fn, device)\n",
    "\n",
    "print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", val_cm)\n",
    "print(\"Classification Report:\\n\", val_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736cb4b7",
   "metadata": {},
   "source": [
    "## Class Weight loss\n",
    "Form the previous observation, the model tries to maximize overall accuracy by predicting only the largest class. We wanted standardize the class weight to prevent the model from excessive punishment small classes.\n",
    "\n",
    "original_weights.max() → normalizes to the heaviest class\n",
    "\n",
    "0.5 + (w * 0.5) → all classes get a minimum weight of 0.5, which prevents over-penalization\n",
    "\n",
    "So the model does not over-weight classes with few samples like \"Noisy\"## Class Weight loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b707ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:  [0.42436975 2.82365591 0.87475017 6.83854167]\n",
      "Scaled weights:  [0.5310278  0.70645161 0.56395736 1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Compute class weights based on the training labels\n",
    "original_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1, 2, 3]),\n",
    "    y=train_df['label'].values\n",
    ")\n",
    "\n",
    "# Normalize and scale the weights\n",
    "scaled_weights = original_weights / original_weights.max()  # normalize to max=1\n",
    "scaled_weights = 0.5 + (scaled_weights * 0.5)  # shrink range to [0.5, 1.0] for balance\n",
    "\n",
    "weights_tensor = torch.tensor(scaled_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Weighted loss function\n",
    "loss_fn_weighted = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "print(\"Original weights: \", original_weights)\n",
    "print(\"Scaled weights: \", scaled_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a76c2e",
   "metadata": {},
   "source": [
    "## Class Weighted Base Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a95e649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.9060 | Train Acc: 0.6409\n",
      "Val   Loss: 0.8320 | Val   Acc: 0.6375\n",
      "----------------------------------------\n",
      "Epoch 2/10\n",
      "Train Loss: 0.8490 | Train Acc: 0.6636\n",
      "Val   Loss: 0.8250 | Val   Acc: 0.6494\n",
      "----------------------------------------\n",
      "Epoch 3/10\n",
      "Train Loss: 0.8202 | Train Acc: 0.6712\n",
      "Val   Loss: 0.8126 | Val   Acc: 0.6710\n",
      "----------------------------------------\n",
      "Epoch 4/10\n",
      "Train Loss: 0.7914 | Train Acc: 0.6883\n",
      "Val   Loss: 0.7730 | Val   Acc: 0.6915\n",
      "----------------------------------------\n",
      "Epoch 5/10\n",
      "Train Loss: 0.7638 | Train Acc: 0.7013\n",
      "Val   Loss: 0.8404 | Val   Acc: 0.6516\n",
      "----------------------------------------\n",
      "Epoch 6/10\n",
      "Train Loss: 0.7207 | Train Acc: 0.7211\n",
      "Val   Loss: 0.7940 | Val   Acc: 0.6602\n",
      "----------------------------------------\n",
      "Epoch 7/10\n",
      "Train Loss: 0.6961 | Train Acc: 0.7289\n",
      "Val   Loss: 0.7463 | Val   Acc: 0.6990\n",
      "----------------------------------------\n",
      "Epoch 8/10\n",
      "Train Loss: 0.6410 | Train Acc: 0.7511\n",
      "Val   Loss: 0.7427 | Val   Acc: 0.7012\n",
      "----------------------------------------\n",
      "Epoch 9/10\n",
      "Train Loss: 0.5869 | Train Acc: 0.7751\n",
      "Val   Loss: 0.7329 | Val   Acc: 0.7001\n",
      "----------------------------------------\n",
      "Epoch 10/10\n",
      "Train Loss: 0.5392 | Train Acc: 0.7887\n",
      "Val   Loss: 0.7657 | Val   Acc: 0.6958\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, loss_fn_weighted)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, loss_fn_weighted)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3889b6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.5583\n",
      "Confusion Matrix:\n",
      " [[461   4  71   8]\n",
      " [ 18  25  36   5]\n",
      " [105  10 142   7]\n",
      " [  7   0  11  17]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.78      0.85      0.81       544\n",
      "          AF       0.64      0.30      0.41        84\n",
      "       Other       0.55      0.54      0.54       264\n",
      "       Noisy       0.46      0.49      0.47        35\n",
      "\n",
      "    accuracy                           0.70       927\n",
      "   macro avg       0.61      0.54      0.56       927\n",
      "weighted avg       0.69      0.70      0.69       927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc, val_f1, val_cm, val_report = evaluate_with_metrics(model, val_loader, loss_fn_weighted, device)\n",
    "\n",
    "print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", val_cm)\n",
    "print(\"Classification Report:\\n\", val_report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8abc738",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning\n",
    "In this section, we aim to improve the performance of our baseline ECG classification model by tuning key hyperparameters. These include:\n",
    "\n",
    "- Learning Rate (lr): Controls how much the model weights are updated during training.\n",
    "\n",
    "- Hidden Size (hidden_size): Determines the capacity of the recurrent layer (GRU), affecting the model’s ability to capture temporal patterns.\n",
    "\n",
    "- Dropout Rate (dropout): Helps prevent overfitting by randomly zeroing some neuron activations during training.\n",
    "\n",
    "- Optimizer Type: Different optimizers like Adam or SGD may converge differently depending on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "50ed20b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(param_grid, train_df, val_df, num_epochs=10):\n",
    "    results = []\n",
    "\n",
    "    for lr in param_grid['lr']:\n",
    "        for hidden_size in param_grid['hidden_size']:\n",
    "            for dropout in param_grid['dropout']:\n",
    "                for opt in param_grid['optimizer']:\n",
    "                    print(f\"\\n Training with lr={lr}, hidden_size={hidden_size}, dropout={dropout}, optimizer={opt}\")\n",
    "\n",
    "                    # Dataset & DataLoader\n",
    "                    train_dataset = ECGDataset(train_df)\n",
    "                    val_dataset = ECGDataset(val_df)\n",
    "                    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,generator=torch.Generator().manual_seed(42))\n",
    "                    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "                    # Model\n",
    "                    model = ECGModel(hidden_size=hidden_size, dropout_rate=dropout).to(device)\n",
    "\n",
    "                    # Optimizer\n",
    "                    if opt == 'adam':\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                    elif opt == 'sgd':\n",
    "                        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "                    else:\n",
    "                        raise ValueError(f\"Unsupported optimizer: {opt}\")\n",
    "\n",
    "                    # Loss\n",
    "                    loss_fn = nn.CrossEntropyLoss(weight=weights_tensor.to(device))\n",
    "\n",
    "                    # Train\n",
    "                    for epoch in range(num_epochs):\n",
    "                        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
    "                        val_loss, val_acc = evaluate(model, val_loader, loss_fn)\n",
    "                        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "                        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
    "\n",
    "                    # Eval\n",
    "                    _, _, val_f1, _, _ = evaluate_with_metrics(model, val_loader, loss_fn, device)\n",
    "\n",
    "                    results.append({\n",
    "                        'lr': lr,\n",
    "                        'hidden_size': hidden_size,\n",
    "                        'dropout': dropout,\n",
    "                        'optimizer': opt,\n",
    "                        'val_f1': val_f1\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39862404",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lr': [0.001, 0.0005],\n",
    "    'hidden_size': [128, 256],\n",
    "    'dropout': [0.2, 0.3],\n",
    "    'optimizer': ['adam', 'sgd']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a9abcda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abd7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training with lr=0.001, hidden_size=128, dropout=0.2, optimizer=adam\n",
      "Train Loss: 1.0921 | Train Acc: 0.5727\n",
      "Val   Loss: 1.0089 | Val   Acc: 0.5707\n",
      "Train Loss: 1.0553 | Train Acc: 0.5933\n",
      "Val   Loss: 0.9607 | Val   Acc: 0.5987\n",
      "Train Loss: 0.9784 | Train Acc: 0.6114\n",
      "Val   Loss: 0.8814 | Val   Acc: 0.6203\n",
      "Train Loss: 0.9233 | Train Acc: 0.6339\n",
      "Val   Loss: 0.9268 | Val   Acc: 0.6127\n",
      "Train Loss: 0.8638 | Train Acc: 0.6662\n",
      "Val   Loss: 0.8754 | Val   Acc: 0.6235\n",
      "Train Loss: 0.8444 | Train Acc: 0.6645\n",
      "Val   Loss: 0.7807 | Val   Acc: 0.6742\n",
      "\n",
      " Training with lr=0.001, hidden_size=128, dropout=0.2, optimizer=sgd\n",
      "Train Loss: 1.1006 | Train Acc: 0.5689\n",
      "Val   Loss: 1.0144 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0778 | Train Acc: 0.5832\n",
      "Val   Loss: 1.0004 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0721 | Train Acc: 0.5836\n",
      "Val   Loss: 0.9951 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0676 | Train Acc: 0.5863\n",
      "Val   Loss: 0.9960 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0627 | Train Acc: 0.5861\n",
      "Val   Loss: 0.9880 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0558 | Train Acc: 0.5859\n",
      "Val   Loss: 0.9846 | Val   Acc: 0.5868\n",
      "\n",
      " Training with lr=0.001, hidden_size=128, dropout=0.3, optimizer=adam\n",
      "Train Loss: 1.0977 | Train Acc: 0.5649\n",
      "Val   Loss: 1.0231 | Val   Acc: 0.5620\n",
      "Train Loss: 1.0899 | Train Acc: 0.5701\n",
      "Val   Loss: 1.0049 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0705 | Train Acc: 0.5754\n",
      "Val   Loss: 0.9720 | Val   Acc: 0.5761\n",
      "Train Loss: 1.0263 | Train Acc: 0.5923\n",
      "Val   Loss: 0.9426 | Val   Acc: 0.5933\n",
      "Train Loss: 0.9603 | Train Acc: 0.6190\n",
      "Val   Loss: 0.8682 | Val   Acc: 0.6332\n",
      "Train Loss: 0.9287 | Train Acc: 0.6388\n",
      "Val   Loss: 0.8685 | Val   Acc: 0.6106\n",
      "\n",
      " Training with lr=0.001, hidden_size=128, dropout=0.3, optimizer=sgd\n",
      "Train Loss: 1.1147 | Train Acc: 0.5554\n",
      "Val   Loss: 1.0143 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0841 | Train Acc: 0.5786\n",
      "Val   Loss: 1.0024 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0783 | Train Acc: 0.5828\n",
      "Val   Loss: 0.9996 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0734 | Train Acc: 0.5828\n",
      "Val   Loss: 0.9992 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0641 | Train Acc: 0.5866\n",
      "Val   Loss: 0.9911 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0591 | Train Acc: 0.5883\n",
      "Val   Loss: 0.9865 | Val   Acc: 0.5868\n",
      "\n",
      " Training with lr=0.001, hidden_size=256, dropout=0.2, optimizer=adam\n",
      "Train Loss: 1.1050 | Train Acc: 0.5632\n",
      "Val   Loss: 1.0306 | Val   Acc: 0.5642\n",
      "Train Loss: 1.0908 | Train Acc: 0.5701\n",
      "Val   Loss: 1.0147 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0831 | Train Acc: 0.5775\n",
      "Val   Loss: 1.0189 | Val   Acc: 0.5642\n",
      "Train Loss: 1.0835 | Train Acc: 0.5788\n",
      "Val   Loss: 1.0244 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0809 | Train Acc: 0.5775\n",
      "Val   Loss: 1.0076 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0722 | Train Acc: 0.5788\n",
      "Val   Loss: 1.0176 | Val   Acc: 0.5868\n",
      "\n",
      " Training with lr=0.001, hidden_size=256, dropout=0.2, optimizer=sgd\n",
      "Train Loss: 1.1044 | Train Acc: 0.5748\n",
      "Val   Loss: 1.0185 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0858 | Train Acc: 0.5838\n",
      "Val   Loss: 1.0065 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0762 | Train Acc: 0.5868\n",
      "Val   Loss: 1.0025 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0693 | Train Acc: 0.5895\n",
      "Val   Loss: 0.9972 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0569 | Train Acc: 0.5893\n",
      "Val   Loss: 0.9782 | Val   Acc: 0.5922\n",
      "Train Loss: 1.0438 | Train Acc: 0.5963\n",
      "Val   Loss: 0.9800 | Val   Acc: 0.5976\n",
      "\n",
      " Training with lr=0.001, hidden_size=256, dropout=0.3, optimizer=adam\n",
      "Train Loss: 1.0998 | Train Acc: 0.5621\n",
      "Val   Loss: 0.9960 | Val   Acc: 0.5717\n",
      "Train Loss: 1.0349 | Train Acc: 0.5933\n",
      "Val   Loss: 0.9453 | Val   Acc: 0.5890\n",
      "Train Loss: 0.9688 | Train Acc: 0.6167\n",
      "Val   Loss: 0.8646 | Val   Acc: 0.6365\n",
      "Train Loss: 0.9363 | Train Acc: 0.6346\n",
      "Val   Loss: 0.9126 | Val   Acc: 0.6203\n",
      "Train Loss: 0.9140 | Train Acc: 0.6502\n",
      "Val   Loss: 0.8694 | Val   Acc: 0.6311\n",
      "Train Loss: 0.9029 | Train Acc: 0.6489\n",
      "Val   Loss: 0.8638 | Val   Acc: 0.6343\n",
      "\n",
      " Training with lr=0.001, hidden_size=256, dropout=0.3, optimizer=sgd\n",
      "Train Loss: 1.0961 | Train Acc: 0.5813\n",
      "Val   Loss: 1.0150 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0791 | Train Acc: 0.5889\n",
      "Val   Loss: 1.0003 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0658 | Train Acc: 0.5893\n",
      "Val   Loss: 0.9977 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0549 | Train Acc: 0.5956\n",
      "Val   Loss: 0.9808 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0406 | Train Acc: 0.6017\n",
      "Val   Loss: 0.9648 | Val   Acc: 0.6084\n",
      "Train Loss: 1.0241 | Train Acc: 0.6076\n",
      "Val   Loss: 0.9675 | Val   Acc: 0.6084\n",
      "\n",
      " Training with lr=0.0005, hidden_size=128, dropout=0.2, optimizer=adam\n",
      "Train Loss: 1.0872 | Train Acc: 0.5769\n",
      "Val   Loss: 1.0227 | Val   Acc: 0.5642\n",
      "Train Loss: 1.0795 | Train Acc: 0.5743\n",
      "Val   Loss: 1.0001 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0555 | Train Acc: 0.5887\n",
      "Val   Loss: 0.9709 | Val   Acc: 0.5771\n",
      "Train Loss: 1.0161 | Train Acc: 0.5963\n",
      "Val   Loss: 0.9690 | Val   Acc: 0.5976\n",
      "Train Loss: 0.9734 | Train Acc: 0.6177\n",
      "Val   Loss: 0.9133 | Val   Acc: 0.6203\n",
      "Train Loss: 0.9398 | Train Acc: 0.6264\n",
      "Val   Loss: 0.8543 | Val   Acc: 0.6365\n",
      "\n",
      " Training with lr=0.0005, hidden_size=128, dropout=0.2, optimizer=sgd\n",
      "Train Loss: 1.1062 | Train Acc: 0.5725\n",
      "Val   Loss: 1.0150 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0869 | Train Acc: 0.5843\n",
      "Val   Loss: 1.0093 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0800 | Train Acc: 0.5864\n",
      "Val   Loss: 1.0116 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0800 | Train Acc: 0.5872\n",
      "Val   Loss: 1.0039 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0725 | Train Acc: 0.5855\n",
      "Val   Loss: 0.9994 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0685 | Train Acc: 0.5882\n",
      "Val   Loss: 0.9948 | Val   Acc: 0.5868\n",
      "\n",
      " Training with lr=0.0005, hidden_size=128, dropout=0.3, optimizer=adam\n",
      "Train Loss: 1.0974 | Train Acc: 0.5661\n",
      "Val   Loss: 1.0192 | Val   Acc: 0.5642\n",
      "Train Loss: 1.0846 | Train Acc: 0.5668\n",
      "Val   Loss: 1.0068 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0823 | Train Acc: 0.5746\n",
      "Val   Loss: 1.0139 | Val   Acc: 0.5599\n",
      "Train Loss: 1.0673 | Train Acc: 0.5762\n",
      "Val   Loss: 0.9856 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0349 | Train Acc: 0.5811\n",
      "Val   Loss: 0.9450 | Val   Acc: 0.6052\n",
      "Train Loss: 1.0041 | Train Acc: 0.6021\n",
      "Val   Loss: 0.9145 | Val   Acc: 0.6063\n",
      "\n",
      " Training with lr=0.0005, hidden_size=128, dropout=0.3, optimizer=sgd\n",
      "Train Loss: 1.1089 | Train Acc: 0.5703\n",
      "Val   Loss: 1.0135 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0950 | Train Acc: 0.5773\n",
      "Val   Loss: 1.0120 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0876 | Train Acc: 0.5765\n",
      "Val   Loss: 1.0092 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0817 | Train Acc: 0.5828\n",
      "Val   Loss: 1.0016 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0786 | Train Acc: 0.5851\n",
      "Val   Loss: 0.9994 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0692 | Train Acc: 0.5906\n",
      "Val   Loss: 0.9958 | Val   Acc: 0.5868\n",
      "\n",
      " Training with lr=0.0005, hidden_size=256, dropout=0.2, optimizer=adam\n",
      "Train Loss: 1.0872 | Train Acc: 0.5743\n",
      "Val   Loss: 1.0036 | Val   Acc: 0.5696\n",
      "Train Loss: 1.0531 | Train Acc: 0.5912\n",
      "Val   Loss: 1.0135 | Val   Acc: 0.5847\n",
      "Train Loss: 0.9964 | Train Acc: 0.6019\n",
      "Val   Loss: 0.9017 | Val   Acc: 0.6192\n",
      "Train Loss: 0.9434 | Train Acc: 0.6291\n",
      "Val   Loss: 0.9300 | Val   Acc: 0.6084\n",
      "Train Loss: 0.8927 | Train Acc: 0.6514\n",
      "Val   Loss: 0.8499 | Val   Acc: 0.6332\n",
      "Train Loss: 0.8734 | Train Acc: 0.6554\n",
      "Val   Loss: 0.7984 | Val   Acc: 0.6688\n",
      "\n",
      " Training with lr=0.0005, hidden_size=256, dropout=0.2, optimizer=sgd\n",
      "Train Loss: 1.1066 | Train Acc: 0.5722\n",
      "Val   Loss: 1.0185 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0833 | Train Acc: 0.5843\n",
      "Val   Loss: 1.0129 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0835 | Train Acc: 0.5878\n",
      "Val   Loss: 1.0167 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0774 | Train Acc: 0.5885\n",
      "Val   Loss: 1.0093 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0746 | Train Acc: 0.5876\n",
      "Val   Loss: 1.0055 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0689 | Train Acc: 0.5885\n",
      "Val   Loss: 1.0006 | Val   Acc: 0.5868\n",
      "\n",
      " Training with lr=0.0005, hidden_size=256, dropout=0.3, optimizer=adam\n",
      "Train Loss: 1.1048 | Train Acc: 0.5615\n",
      "Val   Loss: 1.0194 | Val   Acc: 0.5642\n",
      "Train Loss: 1.0913 | Train Acc: 0.5729\n",
      "Val   Loss: 1.0012 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0484 | Train Acc: 0.5874\n",
      "Val   Loss: 0.9661 | Val   Acc: 0.5771\n",
      "Train Loss: 0.9948 | Train Acc: 0.6026\n",
      "Val   Loss: 0.9824 | Val   Acc: 0.5933\n",
      "Train Loss: 0.9274 | Train Acc: 0.6337\n",
      "Val   Loss: 0.8608 | Val   Acc: 0.6289\n",
      "Train Loss: 0.8869 | Train Acc: 0.6571\n",
      "Val   Loss: 0.8104 | Val   Acc: 0.6602\n",
      "\n",
      " Training with lr=0.0005, hidden_size=256, dropout=0.3, optimizer=sgd\n",
      "Train Loss: 1.1036 | Train Acc: 0.5684\n",
      "Val   Loss: 1.0140 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0850 | Train Acc: 0.5815\n",
      "Val   Loss: 1.0055 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0741 | Train Acc: 0.5885\n",
      "Val   Loss: 1.0043 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0674 | Train Acc: 0.5891\n",
      "Val   Loss: 0.9924 | Val   Acc: 0.5868\n",
      "Train Loss: 1.0588 | Train Acc: 0.5914\n",
      "Val   Loss: 0.9832 | Val   Acc: 0.5901\n",
      "Train Loss: 1.0499 | Train Acc: 0.5929\n",
      "Val   Loss: 0.9742 | Val   Acc: 0.5944\n"
     ]
    }
   ],
   "source": [
    "# It takes approximately 30-40 minutes to run all combinations :)\n",
    "experiment_results = run_experiments(param_grid, train_df, val_df, num_epochs=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f6e8f70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>val_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>0.3</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.526329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.495580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.494430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>256</td>\n",
       "      <td>0.3</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.426548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.3</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.395061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.370311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>128</td>\n",
       "      <td>0.3</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.323067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>256</td>\n",
       "      <td>0.3</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.270581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.219435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>0.3</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.202186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.184908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>128</td>\n",
       "      <td>0.3</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.184908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.184908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>128</td>\n",
       "      <td>0.2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.184908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>128</td>\n",
       "      <td>0.3</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.184908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>256</td>\n",
       "      <td>0.2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>0.184908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lr  hidden_size  dropout optimizer    val_f1\n",
       "14  0.0005          256      0.3      adam  0.526329\n",
       "12  0.0005          256      0.2      adam  0.495580\n",
       "0   0.0010          128      0.2      adam  0.494430\n",
       "6   0.0010          256      0.3      adam  0.426548\n",
       "2   0.0010          128      0.3      adam  0.395061\n",
       "8   0.0005          128      0.2      adam  0.370311\n",
       "10  0.0005          128      0.3      adam  0.323067\n",
       "7   0.0010          256      0.3       sgd  0.270581\n",
       "5   0.0010          256      0.2       sgd  0.219435\n",
       "15  0.0005          256      0.3       sgd  0.202186\n",
       "1   0.0010          128      0.2       sgd  0.184908\n",
       "3   0.0010          128      0.3       sgd  0.184908\n",
       "4   0.0010          256      0.2      adam  0.184908\n",
       "9   0.0005          128      0.2       sgd  0.184908\n",
       "11  0.0005          128      0.3       sgd  0.184908\n",
       "13  0.0005          256      0.2       sgd  0.184908"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_list = experiment_results.sort_values(by=\"val_f1\", ascending=False)\n",
    "top_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e7925c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = ECGModel(hidden_size=256, dropout_rate=0.3).to(device)\n",
    "base_optimizer = torch.optim.Adam(base_model.parameters(), lr=0.0005)\n",
    "base_loss_fn = nn.CrossEntropyLoss(weight=weights_tensor.to(device))\n",
    "train_dataset = ECGDataset(train_df)\n",
    "val_dataset = ECGDataset(val_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,generator=torch.Generator().manual_seed(42))\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1dd45d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: 1.0899 | Train Acc: 0.5746\n",
      "Val   Loss: 1.0089 | Val   Acc: 0.5674\n",
      "----------------------------------------\n",
      "Epoch 2/20\n",
      "Train Loss: 1.0294 | Train Acc: 0.6043\n",
      "Val   Loss: 0.9663 | Val   Acc: 0.5965\n",
      "----------------------------------------\n",
      "Epoch 3/20\n",
      "Train Loss: 0.9697 | Train Acc: 0.6200\n",
      "Val   Loss: 0.8879 | Val   Acc: 0.6224\n",
      "----------------------------------------\n",
      "Epoch 4/20\n",
      "Train Loss: 0.9270 | Train Acc: 0.6379\n",
      "Val   Loss: 0.8835 | Val   Acc: 0.6311\n",
      "----------------------------------------\n",
      "Epoch 5/20\n",
      "Train Loss: 0.8864 | Train Acc: 0.6571\n",
      "Val   Loss: 0.8628 | Val   Acc: 0.6300\n",
      "----------------------------------------\n",
      "Epoch 6/20\n",
      "Train Loss: 0.8864 | Train Acc: 0.6487\n",
      "Val   Loss: 0.8325 | Val   Acc: 0.6289\n",
      "----------------------------------------\n",
      "Epoch 7/20\n",
      "Train Loss: 0.8531 | Train Acc: 0.6643\n",
      "Val   Loss: 0.8062 | Val   Acc: 0.6602\n",
      "----------------------------------------\n",
      "Epoch 8/20\n",
      "Train Loss: 0.8335 | Train Acc: 0.6666\n",
      "Val   Loss: 0.8234 | Val   Acc: 0.6526\n",
      "----------------------------------------\n",
      "Epoch 9/20\n",
      "Train Loss: 0.8278 | Train Acc: 0.6639\n",
      "Val   Loss: 0.8020 | Val   Acc: 0.6559\n",
      "----------------------------------------\n",
      "Epoch 10/20\n",
      "Train Loss: 0.8044 | Train Acc: 0.6835\n",
      "Val   Loss: 0.8790 | Val   Acc: 0.6365\n",
      "----------------------------------------\n",
      "Epoch 11/20\n",
      "Train Loss: 0.7964 | Train Acc: 0.6786\n",
      "Val   Loss: 0.8235 | Val   Acc: 0.6483\n",
      "----------------------------------------\n",
      "Epoch 12/20\n",
      "Train Loss: 0.7736 | Train Acc: 0.6879\n",
      "Val   Loss: 0.7955 | Val   Acc: 0.6753\n",
      "----------------------------------------\n",
      "Epoch 13/20\n",
      "Train Loss: 0.7479 | Train Acc: 0.6944\n",
      "Val   Loss: 0.7399 | Val   Acc: 0.6980\n",
      "----------------------------------------\n",
      "Epoch 14/20\n",
      "Train Loss: 0.7148 | Train Acc: 0.7142\n",
      "Val   Loss: 0.7305 | Val   Acc: 0.7023\n",
      "----------------------------------------\n",
      "Epoch 15/20\n",
      "Train Loss: 0.6898 | Train Acc: 0.7249\n",
      "Val   Loss: 0.7312 | Val   Acc: 0.7120\n",
      "----------------------------------------\n",
      "Epoch 16/20\n",
      "Train Loss: 0.6505 | Train Acc: 0.7376\n",
      "Val   Loss: 0.7839 | Val   Acc: 0.7131\n",
      "----------------------------------------\n",
      "Epoch 17/20\n",
      "Train Loss: 0.6266 | Train Acc: 0.7447\n",
      "Val   Loss: 0.7293 | Val   Acc: 0.7023\n",
      "----------------------------------------\n",
      "Epoch 18/20\n",
      "Train Loss: 0.5834 | Train Acc: 0.7709\n",
      "Val   Loss: 0.7673 | Val   Acc: 0.7044\n",
      "----------------------------------------\n",
      "Epoch 19/20\n",
      "Train Loss: 0.5510 | Train Acc: 0.7820\n",
      "Val   Loss: 0.7285 | Val   Acc: 0.7184\n",
      "----------------------------------------\n",
      "Epoch 20/20\n",
      "Train Loss: 0.4939 | Train Acc: 0.8016\n",
      "Val   Loss: 0.7866 | Val   Acc: 0.6882\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(base_model, train_loader, base_optimizer, base_loss_fn)\n",
    "    val_loss, val_acc = evaluate(base_model, val_loader, base_loss_fn)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7966ef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.6198\n",
      "Confusion Matrix:\n",
      " [[437  14  87   6]\n",
      " [ 14  44  21   5]\n",
      " [ 88  38 129   9]\n",
      " [  3   0   4  28]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.81      0.80      0.80       544\n",
      "          AF       0.46      0.52      0.49        84\n",
      "       Other       0.54      0.49      0.51       264\n",
      "       Noisy       0.58      0.80      0.67        35\n",
      "\n",
      "    accuracy                           0.69       927\n",
      "   macro avg       0.60      0.65      0.62       927\n",
      "weighted avg       0.69      0.69      0.69       927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc, val_f1, val_cm, val_report = evaluate_with_metrics(base_model, val_loader, base_loss_fn, device)\n",
    "\n",
    "print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", val_cm)\n",
    "print(\"Classification Report:\\n\", val_report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
