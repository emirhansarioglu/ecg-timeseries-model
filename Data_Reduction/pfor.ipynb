{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a7f3ba",
   "metadata": {},
   "source": [
    "1. Preprocessor:    \n",
    "    - Filtering\n",
    "    - Augmentation in time domain\n",
    "(still in dataframe)\n",
    "\n",
    "2. Compression\n",
    "    - Store the data in less bits (currently in 16bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea2f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given\n",
    "import struct\n",
    "from pathlib import Path\n",
    "# import RaggedArray type\n",
    "\n",
    "\n",
    "BASE_DIR = Path().resolve()\n",
    "data_path = BASE_DIR / \"data\"  # Directory to store data files\n",
    "train_data_path = BASE_DIR / \"data\" / \"X_train.bin\"\n",
    "test_data_path = BASE_DIR / \"data\" / \"X_test.bin\"\n",
    "train_label_path = BASE_DIR / \"data\" / \"y_train.csv\"\n",
    "\n",
    "\n",
    "def read_binary(path):\n",
    "    ragged_array = []\n",
    "    with open(path, \"rb\") as r:\n",
    "        read_binary_from(ragged_array, r)\n",
    "    return ragged_array\n",
    "\n",
    "\n",
    "def read_binary_from(ragged_array, r):\n",
    "    while True:\n",
    "        size_bytes = r.read(4)\n",
    "        if not size_bytes:\n",
    "            break\n",
    "        sub_array_size = struct.unpack(\"i\", size_bytes)[0]\n",
    "        sub_array = list(\n",
    "            struct.unpack(f\"{sub_array_size}h\", r.read(sub_array_size * 2))\n",
    "        )\n",
    "        sub_array = [int(x) for x in sub_array]\n",
    "        ragged_array.append(sub_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cb36568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a lossless compression type for the data instead of 16bit (e.g., PDICT, PDELTA, PFOR).  --> then NS (null suppression)\n",
    "import dask # we will have a lot of for loops for each data point, so we will use dask to parallelize the process\n",
    "import numpy as np\n",
    "import bitstruct\n",
    "\n",
    "# before patched version\n",
    "# Delta Encoding\n",
    "def delta_encode(data):\n",
    "    deltas = [data[0]]  # Store the first value\n",
    "    for i in range(1, len(data)):\n",
    "        deltas.append(data[i] - data[i - 1])\n",
    "    return deltas\n",
    "\n",
    "# Delta Decoding\n",
    "def delta_decode(deltas):\n",
    "    original = [deltas[0]]  # Start with the first value\n",
    "    for i in range(1, len(deltas)):\n",
    "        original.append(original[-1] + deltas[i])\n",
    "    return original\n",
    "\n",
    "\n",
    "def pack_offsets(offsets, bits_needed=12):\n",
    "    fmt= f\"u{bits_needed}\"\n",
    "    overall_fmt = fmt * len(offsets)\n",
    "    packed = bitstruct.pack(overall_fmt, *offsets)\n",
    "    return packed\n",
    "\n",
    "\n",
    "def unpack_offsets(packed, bits_needed=12, num_values=None):\n",
    "    fmt = f\"u{bits_needed}\"\n",
    "    if num_values is None:\n",
    "        raise ValueError(\"num_values must be provided\")\n",
    "    overall_fmt = fmt * num_values\n",
    "    unpacked = bitstruct.unpack(overall_fmt, packed)\n",
    "    return list(unpacked)\n",
    "\n",
    "\n",
    "def for_encode(data: list) -> tuple:\n",
    "    data_copy = data.copy()\n",
    "    min_val = min(data)\n",
    "    for i in range(len(data_copy)):\n",
    "        data_copy[i] -= min_val\n",
    "        if data_copy[i] < 0:\n",
    "            print(f\"data[i] is negative: {data_copy[i]} min_val: {min_val}\")\n",
    "    return min_val, data_copy\n",
    "\n",
    "def for_decode(data: list, min_val: int) -> list:\n",
    "    return [val + min_val for val in data]\n",
    "\n",
    "\n",
    "def pfor_encode(data: list, patch_size: int = 128) -> tuple:\n",
    "    patches = []\n",
    "    min_vals = []\n",
    "    max_all = 0\n",
    "    bits_needed = 0\n",
    "    for i in range(0, len(data), patch_size):\n",
    "        patch = data[i:i + patch_size]\n",
    "        min_val, encoded_patch = for_encode(patch)\n",
    "\n",
    "        # find least amount of bits to represent the data\n",
    "        max_val = max(encoded_patch)\n",
    "        if max_val > max_all:\n",
    "            bits_needed = max_val.bit_length()\n",
    "            max_all = max_val\n",
    "        patches.append(encoded_patch)\n",
    "        min_vals.append(min_val)\n",
    "\n",
    "    return patches, min_vals, bits_needed\n",
    "\n",
    "\n",
    "def encoded_pfor_to_binary(patches, min_vals, bits_needed=12) -> bytes:\n",
    "    # first 32 bits for the number of patches\n",
    "    # second 32 bits for the bits needed\n",
    "    # third 32 bits for the patch size\n",
    "    # fourth 32 bits for the last patch length\n",
    "    last_patch_len = len(patches[-1])\n",
    "    header = struct.pack(\n",
    "        \"IIII\", len(patches), bits_needed, len(patches[0]), last_patch_len\n",
    "    )\n",
    "\n",
    "    packed_patches = bytearray(header)\n",
    "    for min_val, patch in zip(min_vals, patches):\n",
    "        packed_patches.extend(struct.pack(\"h\", min_val))\n",
    "        packed_offsets = pack_offsets(patch, bits_needed)\n",
    "        packed_patches.extend(packed_offsets)\n",
    "\n",
    "    return bytes(packed_patches)\n",
    "\n",
    "\n",
    "def pfor_decode(patches, patch_size=128) -> list:\n",
    "    decoded_data = []\n",
    "    for min_val, patch in patches:\n",
    "        decoded_patch = for_decode(patch, min_val)\n",
    "        decoded_data.extend(decoded_patch)\n",
    "    return decoded_data\n",
    "\n",
    "\n",
    "def binary_to_decoded_pfor(binary_data: bytes) -> list:\n",
    "    header_size = 16  # 4 integers of 4 bytes each\n",
    "    num_patches, bits_needed, patch_size, last_patch_len = struct.unpack(\n",
    "        \"IIII\", binary_data[:header_size]\n",
    "    )\n",
    "\n",
    "    patches = []\n",
    "    offset = header_size\n",
    "\n",
    "    for i in range(num_patches):\n",
    "        min_val = struct.unpack(\"h\", binary_data[offset : offset + 2])[0]\n",
    "        offset += 2\n",
    "\n",
    "        if i == num_patches - 1:\n",
    "            num_values = last_patch_len\n",
    "        else:\n",
    "            num_values = patch_size\n",
    "\n",
    "        patch_length = (num_values * bits_needed + 7) // 8\n",
    "        packed_offsets = binary_data[offset : offset + patch_length]\n",
    "        offsets = unpack_offsets(packed_offsets, bits_needed, num_values)\n",
    "        offset += patch_length\n",
    "\n",
    "        patches.append((min_val, offsets))\n",
    "\n",
    "    return pfor_decode(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d8de78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First signal length: 3178\n",
      "First signal: [107, 128, 155, 166, 173, 179, 183, 187, 190, 192]...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst signal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msignal[:\u001b[38;5;241m10\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print first 10 values for brevity\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Encode the signal using PFOR\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m encoded_signal, bits_all \u001b[38;5;241m=\u001b[39m pfor_encode(signal, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded signal length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(encoded_signal)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBits used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbits_all\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "signals = read_binary(train_data_path) # ragged array of signals\n",
    "labels = pd.read_csv(train_label_path, header=None)\n",
    "\n",
    "\n",
    "signal_samples = signals[15:20] \n",
    "bits = []\n",
    "min_bits_all = 16\n",
    "for patch_size in [1024*(i+1) for i in range(10)]:\n",
    "    max_bits_patch = 0\n",
    "    for signal in signal_samples:\n",
    "        print(f\"First signal length: {len(signal)}\")\n",
    "        print(f\"First signal: {signal[:10]}...\")  # Print first 10 values for brevity\n",
    "        # Encode the signal using PFOR\n",
    "        encoded_signal, bits_all = pfor_encode(signal, patch_size=128)\n",
    "        print(f\"Encoded signal length: {len(encoded_signal)}\")\n",
    "        print(f\"Bits used: {bits_all}\")\n",
    "        # Decode the signal using PFOR\n",
    "        decoded_signal = pfor_decode(encoded_signal, patch_size=128)\n",
    "        assert signal == decoded_signal, \"Decoded signal does not match original\"\n",
    "        print(f\"Decoded signal length: {len(decoded_signal)}\")\n",
    "        print(f\"Decoded signal: {decoded_signal[:10]}...\")  # Print first 10 values for brevity\n",
    "        if bits_all > max_bits_patch:\n",
    "            max_bits_patch = bits_all\n",
    "    print(f\"Max bits used for patch size {patch_size}: {max_bits_patch}\")\n",
    "    bits.append((patch_size, max_bits_patch))\n",
    "    if max_bits_patch < min_bits_all:\n",
    "        min_bits_all = max_bits_patch\n",
    "\n",
    "print(f\"Min bits used for all patch sizes: {min_bits_all}\")\n",
    "for (size, bit) in bits:\n",
    "    print(f\"size:{size}, bits: {bit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60b30274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signal length: 9000\n",
      "Encoded signal length for patch size 512: 11302\n"
     ]
    }
   ],
   "source": [
    "# test binary conversion\n",
    "signal = signals[0]\n",
    "print(f\"signal length: {len(signal)}\")\n",
    "patch_size = 512\n",
    "\n",
    "encoded_patches, min_vals, _ = pfor_encode(signal, patch_size=patch_size)\n",
    "encoded_signal = encoded_pfor_to_binary(encoded_patches, min_vals, 10)\n",
    "print(f\"Encoded signal length for patch size {patch_size}: {len(encoded_signal)}\")\n",
    "decoded_signal = binary_to_decoded_pfor(encoded_signal)\n",
    "assert signal == decoded_signal, \"Decoded signal does not match original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474f360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515192d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
