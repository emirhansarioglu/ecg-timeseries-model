{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "f7979d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set for reproducibility.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "val_df = pd.read_pickle(\"val_df.pkl\")\n",
    "\n",
    "INPUT_LENGTH = 9000\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0\n",
    "NUM_CLASSES = 4\n",
    "EPOCHS = 10\n",
    "N_FFT = 256\n",
    "HOP_LENGTH = 128\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)  # Python random\n",
    "    np.random.seed(seed)  # NumPy\n",
    "    torch.manual_seed(seed)  # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "print(\"Seed set for reproducibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "e107ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation functions\n",
    "import scipy.signal\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "# wrapping them in nn.Module should not introduce huge overhead\n",
    "\n",
    "\n",
    "def time_shift(signal, shift_range=(-100, 100)):\n",
    "    shift = np.roll(signal, shift=random.randint(shift_range[0], shift_range[1]))\n",
    "    return shift\n",
    "\n",
    "\n",
    "def add_noise(signal, noise_level=0.1):\n",
    "    noise = np.random.normal(0, noise_level, signal.shape)\n",
    "    return signal + noise\n",
    "\n",
    "\n",
    "def time_warp(signal, warp_factor=0.1):\n",
    "    return scipy.signal.resample(\n",
    "        signal, int(len(signal) * (1 + np.random.uniform(-warp_factor, warp_factor)))\n",
    "    )\n",
    "\n",
    "\n",
    "def amplitude_scaling(signal, scale_range=(0.8, 1.2)):\n",
    "    scale = random.uniform(scale_range[0], scale_range[1])\n",
    "    return signal * scale\n",
    "\n",
    "\n",
    "def augment(signal, augmentation=\"all\"):\n",
    "    if augmentation in (\"all\", \"warp_only\") and np.random.rand() < 0.5:\n",
    "        signal = time_warp(signal)\n",
    "    if augmentation in (\"all\", \"noise_only\") and np.random.rand() < 0.5:\n",
    "        signal = add_noise(signal)\n",
    "    if augmentation in (\"all\", \"shift_only\") and np.random.rand() < 0.5:\n",
    "        signal = time_shift(signal)\n",
    "    if augmentation in (\"all\", \"scale_only\") and np.random.rand() < 0.5:\n",
    "        signal = amplitude_scaling(signal)\n",
    "    return signal\n",
    "\n",
    "\n",
    "class ECGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, augmentation=None):\n",
    "        self.df = df\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal = self.df.iloc[idx][\"signal\"]\n",
    "        signal = np.array(signal, dtype=np.float32)\n",
    "        signal = augment(signal, self.augmentation) \n",
    "        signal = torch.tensor(signal, dtype=torch.float32)\n",
    "        \n",
    "        label = int(self.df.iloc[idx][\"label\"])\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        length = len(signal)\n",
    "\n",
    "        return signal, label, length\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO: Optimization: sort by length\n",
    "    signals, labels, lengths = zip(*batch)\n",
    "    signals = [s.unsqueeze(-1) if s.dim() == 1 else s for s in signals]\n",
    "    packed_signals = pack_sequence(signals, enforce_sorted=False)\n",
    "\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return packed_signals, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "1882f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def batch_apply_stft(\n",
    "    signals: torch.Tensor, \n",
    "    lengths: torch.Tensor, \n",
    "    n_fft: int = N_FFT,  \n",
    "    hop_length: int = HOP_LENGTH,  \n",
    "    pad_mode: str = \"constant\"\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    batch_stft = torch.stft(\n",
    "        signals.squeeze(-1),\n",
    "        n_fft,\n",
    "        hop_length,\n",
    "        window=torch.hann_window(n_fft, device=signals.device),\n",
    "        pad_mode=pad_mode,\n",
    "        return_complex=True,\n",
    "    )\n",
    "    magnitude = torch.abs(batch_stft)\n",
    "\n",
    "    new_lengths = torch.clamp((lengths - n_fft) // hop_length + 1, min=1)\n",
    "\n",
    "    return magnitude, new_lengths\n",
    "\n",
    "def he_init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "69307b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data parallelism support\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "\n",
    "\n",
    "class VorgabeRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=50,\n",
    "        num_layers=2,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        n_fft=512,\n",
    "        hop_length=256,\n",
    "        dropout_rate=0.2,\n",
    "    ):\n",
    "        super(VorgabeRNN, self).__init__()\n",
    "\n",
    "        # STFT parameters\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_freqs = n_fft // 2 + 1 \n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.conv_output_size = 64 * (self.n_freqs // 4)  \n",
    "\n",
    "        self.rnn = nn.RNN(self.conv_output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.stft = batch_apply_stft\n",
    "        self.apply(he_init_weights)\n",
    "\n",
    "        self.rnn_type = type(self.rnn).__name__\n",
    "\n",
    "    def feature_extractor(self, x: PackedSequence, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        lengths = lengths.detach().clone().cpu()  \n",
    "\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        x, lengths = self.stft(x, lengths, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "\n",
    "        x = torch.log2(x + 1e-8)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        batch_size, channels, freq_bins, time_frames = x.shape\n",
    "        x = x.view(batch_size, time_frames, -1)  \n",
    "\n",
    "        lengths = lengths // 4\n",
    "        lengths = torch.clamp(lengths, min=1)\n",
    "\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            LSTM_output, LSTM_states = self.rnn(x)\n",
    "            return LSTM_states[0][-1]\n",
    "        else:\n",
    "            RNN_output, RNN_states = self.rnn(x)\n",
    "            return RNN_states[-1]\n",
    "\n",
    "    def forward(self, x: PackedSequence, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.feature_extractor(x, lengths)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x: PackedSequence, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            x = self.forward(x, lengths)\n",
    "            _, predicted = torch.max(x, 1)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "6451fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VorgabeLSTM(VorgabeRNN):\n",
    "    def __init__(self, hidden_size=50, num_layers=2, num_classes=NUM_CLASSES, n_fft=512, hop_length=256, dropout_rate=0.2):\n",
    "        super().__init__(hidden_size, num_layers, num_classes, n_fft, hop_length, dropout_rate)\n",
    "        self.rnn = nn.LSTM(self.conv_output_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "class VorgabeGRU(VorgabeRNN):\n",
    "    def __init__(self, hidden_size=50, num_layers=2, num_classes=NUM_CLASSES, n_fft=512, hop_length=256, dropout_rate=0.2):\n",
    "        super().__init__(hidden_size, num_layers, num_classes, n_fft, hop_length, dropout_rate)\n",
    "        self.rnn = nn.GRU(self.conv_output_size, hidden_size, num_layers, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "f0caa10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm\n",
    "class RNNwithSVM(VorgabeRNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=50,\n",
    "        num_layers=2,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        n_fft=512,\n",
    "        hop_length=256,\n",
    "        dropout_rate=0.2,\n",
    "    ):\n",
    "        super(RNNwithSVM, self).__init__(\n",
    "            hidden_size, num_layers, num_classes, n_fft, hop_length, dropout_rate\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.svm = sklearn.svm.SVC()\n",
    "        self.is_svm_trained = False\n",
    "\n",
    "    def forward(self, x: PackedSequence, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.feature_extractor(x, lengths)[-1]\n",
    "        x = self.fc1(x)\n",
    "        if self.fc2 is not None and not self.is_svm_trained:\n",
    "            return self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x: PackedSequence, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.is_svm_trained:\n",
    "            raise ValueError(\"SVM not trained yet!\")\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            x = self.feature_extractor(x, lengths)[-1]\n",
    "            x = self.fc1(x)\n",
    "            x = self.svm.predict(x.cpu().numpy())\n",
    "        return x\n",
    "    def _remove_temp_classifier(self):\n",
    "        self.fc2 = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "99f21593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VorgabeRNN output shape: tensor([[ 0.9566,  0.8288,  0.1050,  1.0976],\n",
      "        [ 1.8364, -1.1014,  0.2536,  0.8392],\n",
      "        [ 1.1846,  0.2229, -0.1981, -0.1515],\n",
      "        [ 0.0820,  0.5633, -0.0595,  0.7985],\n",
      "        [ 0.6554,  0.6080, -0.1672, -0.3661],\n",
      "        [-0.2211,  1.3016,  0.4257,  0.3523],\n",
      "        [ 0.2991,  0.4833,  0.2282, -1.0839],\n",
      "        [ 1.7641,  0.8308, -0.4762, -0.9565],\n",
      "        [ 0.9965,  0.6118,  0.2116,  0.1423],\n",
      "        [ 1.1346,  0.6552, -0.2813,  0.1996],\n",
      "        [ 1.0159,  0.3122,  0.1073, -0.1924],\n",
      "        [ 0.9255, -0.2005, -0.4589, -0.0102],\n",
      "        [ 1.3453,  0.3117, -0.0794,  0.5009],\n",
      "        [-0.7877, -0.2351,  0.5842,  0.7118],\n",
      "        [ 0.8150, -0.1024, -0.4068, -0.2260],\n",
      "        [ 0.5930,  0.6425, -0.2433,  0.5619],\n",
      "        [ 1.2405, -0.3554, -0.0829,  0.1963],\n",
      "        [ 1.2476,  0.4571,  0.0409, -0.3896],\n",
      "        [ 1.1591,  0.6879, -0.5779, -0.6107],\n",
      "        [ 1.2506,  0.6973, -0.2269, -0.2035],\n",
      "        [-1.4511,  1.0659,  0.2453,  0.8410],\n",
      "        [ 1.7897,  1.1880,  0.6511,  0.4295],\n",
      "        [ 1.2786, -0.1177, -0.5202,  0.4803],\n",
      "        [ 0.8930, -0.2352, -0.0128, -0.1582],\n",
      "        [ 0.1280,  1.1006,  0.1417, -0.0340],\n",
      "        [ 1.0656,  0.6969,  0.0945, -0.4637],\n",
      "        [ 1.2325,  0.4942,  0.1516,  0.0259],\n",
      "        [ 0.7091,  0.2858, -0.6656, -0.7279],\n",
      "        [ 1.7519, -0.0568,  0.2944,  1.1217],\n",
      "        [ 0.3195,  0.6414, -0.0347, -0.0871],\n",
      "        [ 0.5771,  1.3588, -0.1829,  0.4583],\n",
      "        [-0.6498,  1.2658,  0.2378,  0.4531],\n",
      "        [ 0.5567,  1.5472,  0.0480, -0.2197],\n",
      "        [ 1.5022,  0.6271,  0.2376,  0.9753],\n",
      "        [ 1.4930,  0.0503,  0.1483,  0.4396],\n",
      "        [-0.3560, -0.1564, -0.0678,  0.2555],\n",
      "        [ 0.2741, -0.3962, -0.3323,  0.5903],\n",
      "        [ 0.0887,  0.4803, -0.3880,  0.5469],\n",
      "        [ 0.7773, -0.0543, -0.2024,  0.3886],\n",
      "        [ 0.6321,  0.3296, -0.1971, -0.8812],\n",
      "        [ 0.6245,  0.5370,  0.1843, -0.5284],\n",
      "        [ 1.3666,  0.9200,  0.3313, -1.2526],\n",
      "        [ 0.8390,  0.0231,  0.1752, -0.0119],\n",
      "        [ 1.4431,  0.5059, -0.1130, -0.9458],\n",
      "        [ 1.3052,  1.0600, -0.7310,  0.9932],\n",
      "        [ 0.7231,  0.0592, -0.3451,  0.3058],\n",
      "        [ 0.5185,  0.7246, -0.1098, -0.8589],\n",
      "        [ 0.6241,  0.2352,  0.0227, -0.8797],\n",
      "        [-0.6140,  0.3087,  0.1161, -0.5028],\n",
      "        [ 0.6667,  0.4605, -0.7696, -0.0564],\n",
      "        [ 0.9483,  0.4752, -0.4673,  0.2600],\n",
      "        [-1.2243, -0.0155, -0.6086, -0.1948],\n",
      "        [-0.8624,  1.0572, -0.1075,  0.3392],\n",
      "        [ 0.6572,  0.3887, -0.1266, -0.4078],\n",
      "        [ 0.4921,  0.8024, -0.2806, -0.8791],\n",
      "        [ 1.0709,  0.3801, -0.5664,  0.6047],\n",
      "        [ 0.9744,  0.3968, -0.2453,  0.2158],\n",
      "        [ 0.5759,  0.6989, -0.6387, -0.8192],\n",
      "        [ 0.5753,  1.1991, -0.1596, -0.8958],\n",
      "        [-1.1303,  0.9334,  0.4725,  1.1779],\n",
      "        [-0.4186,  0.0939, -0.0970, -0.3571],\n",
      "        [ 0.9092,  0.6616,  0.2440, -0.4993],\n",
      "        [ 0.1282, -0.1746,  0.5940,  0.4778],\n",
      "        [ 0.0162,  0.7453, -0.0063,  0.0726]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ECGDataset(train_df, augmentation=None)\n",
    "val_dataset = ECGDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "sample_signal, sample_label, sample_length = next(iter(train_loader))\n",
    "vorgabe_rnn = VorgabeRNN()\n",
    "output = vorgabe_rnn(sample_signal, sample_length)\n",
    "print(f\"VorgabeRNN output shape: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "40b9baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, epochs=10, lr=0.001, batch_size_factor=4):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "    model.to(device)\n",
    "\n",
    "    is_nn_svm = hasattr(model, \"svm\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    original_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1, 2, 3]),\n",
    "    y=train_df['label'].values\n",
    ")\n",
    "\n",
    "    # Normalize and scale the weights\n",
    "    scaled_weights = original_weights / original_weights.max()  # normalize to max=1\n",
    "    scaled_weights = 0.5 + (scaled_weights * 0.5)  # shrink range to [0.5, 1.0] for balance\n",
    "\n",
    "    weights_tensor = torch.tensor(scaled_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "    print(\"Training RNN feature extractor...\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for signals, labels, lengths in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            signals, labels, lengths = signals.to(device), labels.to(device), lengths.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(signals, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        acc = 100.0 * correct / total\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}: Train Loss: {total_loss/len(train_loader):.4f}, Train Acc: {acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "    if is_nn_svm:\n",
    "        model._remove_temp_classifier()\n",
    "        _train_svm_phase(model, train_loader, device, batch_size_factor) \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def _train_svm_phase(model, train_loader, device, batch_size_factor):\n",
    "\n",
    "    print(\"Training SVM...\")\n",
    "    model.eval()\n",
    "    all_features, all_labels = [], []\n",
    "    batch_features, batch_labels = [], []\n",
    "    assert model.fc2 is None, \"Temporary classifier should be removed before SVM training\"\n",
    "\n",
    "    # extract_features_from_loader(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        for signals, labels, lengths in tqdm(train_loader, desc=\"Extracting features\"):\n",
    "            signals, lengths = signals.to(device), lengths.to(device)\n",
    "            features = model(signals, lengths)\n",
    "\n",
    "            batch_features.append(features)\n",
    "            batch_labels.append(labels)\n",
    "\n",
    "            if len(batch_features) >= batch_size_factor:\n",
    "                combined_features = torch.cat(batch_features, dim=0).cpu().numpy()\n",
    "                combined_labels = torch.cat(batch_labels, dim=0).numpy()\n",
    "                all_features.append(combined_features)\n",
    "                all_labels.append(combined_labels)\n",
    "                batch_features, batch_labels = [], []\n",
    "\n",
    "\n",
    "    if batch_features:\n",
    "        combined_features = torch.cat(batch_features, dim=0).cpu().numpy()\n",
    "        combined_labels = torch.cat(batch_labels, dim=0).numpy()\n",
    "        all_features.append(combined_features)\n",
    "        all_labels.append(combined_labels)\n",
    "\n",
    "    final_features = np.concatenate(all_features, axis=0)\n",
    "    final_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    print(f\"Training SVM on {len(final_features)} samples\")\n",
    "    model.svm.fit(final_features, final_labels)\n",
    "    model.is_svm_trained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "2eb95fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    print(\"Evaluating model...\")\n",
    "    model.eval()\n",
    "    all_predictions, all_labels = [], []\n",
    "    is_nn_svm = hasattr(model, \"svm\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for signals, labels, lengths in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            signals, lengths = (\n",
    "                signals.to(device),\n",
    "                lengths.to(device),\n",
    "            )\n",
    "\n",
    "            \n",
    "            predictions = model.predict(signals, lengths)\n",
    "            if not is_nn_svm:\n",
    "                predictions = predictions.cpu().numpy()\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Final Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    # normalize heatmap colors in each row for class distribution\n",
    "    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c9c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self : __torch__.___torch_mangle_71.VorgabeRNN,\n",
      "      %x.1 : NamedTuple(data : Tensor, batch_sizes : Tensor, sorted_indices : Tensor?, unsorted_indices : Tensor?),\n",
      "      %lengths.1 : Tensor):\n",
      "  %x.5 : Tensor = prim::CallMethod[name=\"feature_extractor\"](%self, %x.1, %lengths.1) # /var/folders/8y/9y3t89w944n3vzl3tq_mzvs80000gn/T/ipykernel_66268/300297223.py:71:12\n",
      "  %fc : __torch__.torch.nn.modules.linear.___torch_mangle_37.Linear = prim::GetAttr[name=\"fc\"](%self)\n",
      "  %x.9 : Tensor = prim::CallMethod[name=\"forward\"](%fc, %x.5) # /var/folders/8y/9y3t89w944n3vzl3tq_mzvs80000gn/T/ipykernel_66268/300297223.py:72:12\n",
      "  return (%x.9)\n",
      "\n",
      "graph(%self : __torch__.___torch_mangle_72.VorgabeLSTM,\n",
      "      %x.1 : NamedTuple(data : Tensor, batch_sizes : Tensor, sorted_indices : Tensor?, unsorted_indices : Tensor?),\n",
      "      %lengths.1 : Tensor):\n",
      "  %x.5 : Tensor = prim::CallMethod[name=\"feature_extractor\"](%self, %x.1, %lengths.1) # /var/folders/8y/9y3t89w944n3vzl3tq_mzvs80000gn/T/ipykernel_66268/300297223.py:71:12\n",
      "  %fc : __torch__.torch.nn.modules.linear.___torch_mangle_37.Linear = prim::GetAttr[name=\"fc\"](%self)\n",
      "  %x.9 : Tensor = prim::CallMethod[name=\"forward\"](%fc, %x.5) # /var/folders/8y/9y3t89w944n3vzl3tq_mzvs80000gn/T/ipykernel_66268/300297223.py:72:12\n",
      "  return (%x.9)\n",
      "\n",
      "graph(%self : __torch__.___torch_mangle_73.VorgabeGRU,\n",
      "      %x.1 : NamedTuple(data : Tensor, batch_sizes : Tensor, sorted_indices : Tensor?, unsorted_indices : Tensor?),\n",
      "      %lengths.1 : Tensor):\n",
      "  %x.5 : Tensor = prim::CallMethod[name=\"feature_extractor\"](%self, %x.1, %lengths.1) # /var/folders/8y/9y3t89w944n3vzl3tq_mzvs80000gn/T/ipykernel_66268/300297223.py:71:12\n",
      "  %fc : __torch__.torch.nn.modules.linear.___torch_mangle_37.Linear = prim::GetAttr[name=\"fc\"](%self)\n",
      "  %x.9 : Tensor = prim::CallMethod[name=\"forward\"](%fc, %x.5) # /var/folders/8y/9y3t89w944n3vzl3tq_mzvs80000gn/T/ipykernel_66268/300297223.py:72:12\n",
      "  return (%x.9)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import onnx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_shape = (BATCH_SIZE, INPUT_LENGTH, 1)\n",
    "\n",
    "for model in [VorgabeRNN, VorgabeLSTM, VorgabeGRU]:\n",
    "\n",
    "    model_instance = model(\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        n_fft=256,\n",
    "        hop_length=128,\n",
    "        dropout_rate=0.2,\n",
    "    )\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_instance = model_instance.to(device)\n",
    "    model_instance.eval()\n",
    "    device = \"mps\"\n",
    "    dummy_input = torch.randn(*input_shape, device=device)\n",
    "    dummy_input = pack_sequence(\n",
    "        [dummy_input] * input_shape[0], enforce_sorted=False\n",
    "    )  \n",
    "    dummy_lengths = torch.tensor(\n",
    "        [input_shape[1]] * input_shape[0], dtype=torch.int64, device=device\n",
    "    )\n",
    "    scripted = torch.jit.script(model_instance)\n",
    "    print(scripted.graph)\n",
    "    scripted.save(f\"{model.__name__}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fa00fa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for rnn.weight_hh_l0 are zero.\n",
      "Gradients for rnn.weight_hh_l1 are zero.\n",
      "Gradients for rnn.weight_hh_l0 are zero.\n",
      "Gradients for rnn.weight_hh_l1 are zero.\n",
      "Gradients for rnn.weight_hh_l0 are zero.\n",
      "Gradients for rnn.weight_hh_l1 are zero.\n"
     ]
    }
   ],
   "source": [
    "def check_gradients(model):\n",
    "    model.train()  \n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    dummy_sequences = []\n",
    "    for i in range(BATCH_SIZE):  \n",
    "        seq = torch.randn(\n",
    "            1000, 1, device=device, requires_grad=True\n",
    "        )\n",
    "        dummy_sequences.append(seq)\n",
    "\n",
    "    dummy_input = pack_sequence(dummy_sequences, enforce_sorted=False)\n",
    "    dummy_lengths = torch.tensor(\n",
    "        [1000] * BATCH_SIZE, dtype=torch.int64, device=device\n",
    "    )\n",
    "\n",
    "    output = model(dummy_input, dummy_lengths)\n",
    "    loss = output.sum()  \n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            if param.grad.abs().sum() == 0:\n",
    "                print(f\"Gradients for {name} are zero.\")\n",
    "\n",
    "\n",
    "for model in [VorgabeRNN, VorgabeLSTM, VorgabeGRU]:\n",
    "    model_instance = model(\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        n_fft=256,\n",
    "        hop_length=128,\n",
    "        dropout_rate=0.2,\n",
    "    )\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_instance = model_instance.to(device)\n",
    "    check_gradients(model_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133f80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tests with different configurations...\n",
      "Testing VorgabeLSTM with all\n",
      "Training RNN feature extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 83/83 [01:18<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.1139, Train Acc: 58.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 83/83 [01:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.0830, Train Acc: 58.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 83/83 [00:57<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 1.0788, Train Acc: 58.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 83/83 [00:57<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 1.0739, Train Acc: 58.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 83/83 [00:43<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 1.0737, Train Acc: 58.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 83/83 [00:41<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 1.0685, Train Acc: 58.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 83/83 [00:44<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 1.0714, Train Acc: 58.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 83/83 [00:44<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 1.0661, Train Acc: 58.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 83/83 [00:50<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 1.0680, Train Acc: 58.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 83/83 [00:47<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 1.0644, Train Acc: 58.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 83/83 [01:02<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 1.0612, Train Acc: 58.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 83/83 [00:50<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 1.0622, Train Acc: 58.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 83/83 [00:42<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 1.0629, Train Acc: 58.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 83/83 [00:46<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 1.0701, Train Acc: 58.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 83/83 [00:52<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 1.0697, Train Acc: 58.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 83/83 [00:50<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 1.0778, Train Acc: 58.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:  58%|█████▊    | 48/83 [00:26<00:19,  1.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[254], line 53\u001b[0m\n\u001b[1;32m     42\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     43\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     44\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m model_instance\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(trained_model, val_loader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maugmentation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[251], line 42\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, epochs, lr, batch_size_factor)\u001b[0m\n\u001b[1;32m     40\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(signals, lengths)\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     45\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Uni/AMLS/ecg-timeseries-model/.venv/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Uni/AMLS/ecg-timeseries-model/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Uni/AMLS/ecg-timeseries-model/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_space = {\n",
    "    \"model\": [VorgabeLSTM, VorgabeGRU, VorgabeRNN],\n",
    "    \"augmentation\": [\"all\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "print(\"Starting tests with different configurations...\")\n",
    "\n",
    "for aug in test_space[\"augmentation\"]:\n",
    "    train_set = ECGDataset(train_df, augmentation=aug)\n",
    "    val_set = ECGDataset(val_df)\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, generator=torch.Generator().manual_seed(42), num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
    "\n",
    "    for model_cls in test_space[\"model\"]:\n",
    "        print(f\"Testing {model_cls.__name__} with {aug}\")\n",
    "        model = model_cls(hidden_size=128, \n",
    "                          num_layers=2, \n",
    "                          num_classes=NUM_CLASSES, \n",
    "                          n_fft=N_FFT, \n",
    "                          hop_length=HOP_LENGTH, \n",
    "                          dropout_rate=0.2)\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "        model = model.to(device)\n",
    "        trained = train_model(model, train_loader, epochs=EPOCHS, lr=0.001, batch_size_factor=4)\n",
    "        acc = evaluate_model(trained, val_loader, device)\n",
    "\n",
    "        print(f\"Validation Accuracy of {model_cls.__name__} with {aug}: {acc:.4f}\\n\")\n",
    "        results.append({\"model\": model_cls.__name__, \"augmentation\": aug, \"accuracy\": acc})\n",
    "        del trained\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    del train_set, val_set, train_loader, val_loader\n",
    "\n",
    "print(\"All tests completed.\")\n",
    "best = max(results, key=lambda x: x[\"accuracy\"])\n",
    "print(f\"Best Model: {best['model']}, Augmentation: {best['augmentation']}, Accuracy: {best['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbdbc88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002db5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amls-project-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
