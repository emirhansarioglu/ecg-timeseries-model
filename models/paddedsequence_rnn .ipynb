{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7979d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set for reproducibility.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Modify this for your machine\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "MAIN_PATH = (\n",
    "    \"/content/drive/MyDrive/ecg-timeseries-model/models/\" if DEVICE == \"cuda\" else \"\"\n",
    ")\n",
    "\n",
    "train_df = pd.read_pickle(MAIN_PATH + \"train_df.pkl\")\n",
    "val_df = pd.read_pickle(MAIN_PATH + \"val_df.pkl\")\n",
    "\n",
    "INPUT_LENGTH = 9000\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "NUM_WORKERS = 8 if DEVICE == \"cuda\" else 0\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "CLASS_NAMES = {0: \"Normal\", 1: \"AF\", 2: \"Other\", 3: \"Noisy\"}\n",
    "\n",
    "\n",
    "TRAINING_MEAN = 1.07e-09\n",
    "TRAINING_STD = 175.11\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)  # Python random\n",
    "    np.random.seed(seed)  # NumPy\n",
    "    torch.manual_seed(seed)  # PyTorch CPU\n",
    "    torch.cuda.manual_seed(seed)  # PyTorch GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "print(\"Seed set for reproducibility.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "e107ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation functions\n",
    "import scipy.signal\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "# wrapping them in nn.Module should not introduce huge overhead\n",
    "\n",
    "\n",
    "def time_shift(signal, shift_range=(-100, 100)):\n",
    "    shift = np.roll(signal, shift=random.randint(shift_range[0], shift_range[1]))\n",
    "    return shift\n",
    "\n",
    "\n",
    "def add_noise(signal, noise_level=0.1):\n",
    "    noise = np.random.normal(0, noise_level, signal.shape)\n",
    "    return signal + noise\n",
    "\n",
    "\n",
    "def time_warp(signal, warp_factor=0.1):\n",
    "    return scipy.signal.resample(\n",
    "        signal, int(len(signal) * (1 + np.random.uniform(-warp_factor, warp_factor)))\n",
    "    )\n",
    "\n",
    "\n",
    "def amplitude_scaling(signal, scale_range=(0.8, 1.2)):\n",
    "    scale = random.uniform(scale_range[0], scale_range[1])\n",
    "    return signal * scale\n",
    "\n",
    "\n",
    "def augment(signal, augmentation=\"all\"):\n",
    "    if augmentation in (\"all\", \"warp_only\") and np.random.rand() < 0.5:\n",
    "        signal = time_warp(signal)\n",
    "    if augmentation in (\"all\", \"noise_only\") and np.random.rand() < 0.5:\n",
    "        signal = add_noise(signal)\n",
    "    if augmentation in (\"all\", \"shift_only\") and np.random.rand() < 0.5:\n",
    "        signal = time_shift(signal)\n",
    "    if augmentation in (\"all\", \"scale_only\") and np.random.rand() < 0.5:\n",
    "        signal = amplitude_scaling(signal)\n",
    "    return signal\n",
    "\n",
    "\n",
    "class ECGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, augmentation=None):\n",
    "        self.df = df\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        signal = self.df.iloc[idx][\"signal\"]\n",
    "        signal = np.array(signal, dtype=np.float32)\n",
    "        signal = augment(signal, self.augmentation) \n",
    "        signal = torch.tensor(signal, dtype=torch.float32)\n",
    "        \n",
    "        label = int(self.df.iloc[idx][\"label\"])\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        length = len(signal)\n",
    "\n",
    "        return signal, label, length\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # TODO: Optimization: sort by length\n",
    "    signals, labels, lengths = zip(*batch)\n",
    "    signals = [s.unsqueeze(-1) if s.dim() == 1 else s for s in signals]\n",
    "    packed_signals = pack_sequence(signals, enforce_sorted=False)\n",
    "\n",
    "    lengths = torch.tensor(lengths, dtype=torch.int64)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    return packed_signals, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "1882f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def batch_apply_stft(\n",
    "    signals: torch.Tensor, \n",
    "    lengths: torch.Tensor, \n",
    "    n_fft: int = N_FFT,  \n",
    "    hop_length: int = HOP_LENGTH,  \n",
    "    pad_mode: str = \"constant\"\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    batch_stft = torch.stft(\n",
    "        signals.squeeze(-1),\n",
    "        n_fft,\n",
    "        hop_length,\n",
    "        window=torch.hann_window(n_fft, device=signals.device),\n",
    "        pad_mode=pad_mode,\n",
    "        return_complex=True,\n",
    "    )\n",
    "    magnitude = torch.abs(batch_stft)\n",
    "\n",
    "    new_lengths = torch.clamp((lengths - n_fft) // hop_length + 1, min=1)\n",
    "\n",
    "    return magnitude, new_lengths\n",
    "\n",
    "def he_init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "69307b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data parallelism support\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import PackedSequence\n",
    "\n",
    "\n",
    "class VorgabeRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=50,\n",
    "        num_layers=2,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        n_fft=512,\n",
    "        hop_length=256,\n",
    "        dropout_rate=0.2,\n",
    "    ):\n",
    "        super(VorgabeRNN, self).__init__()\n",
    "\n",
    "        # STFT parameters\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.n_freqs = n_fft // 2 + 1 \n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.conv_output_size = 64 * (self.n_freqs // 4)  \n",
    "\n",
    "        self.rnn = nn.RNN(self.conv_output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.stft = batch_apply_stft\n",
    "        self.apply(he_init_weights)\n",
    "\n",
    "        self.rnn_type = type(self.rnn).__name__\n",
    "\n",
    "    def feature_extractor(self, x: PackedSequence, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        lengths = lengths.detach().clone().cpu()  \n",
    "\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        x, lengths = self.stft(x, lengths, n_fft=self.n_fft, hop_length=self.hop_length)\n",
    "\n",
    "        x = torch.log2(x + 1e-8)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        batch_size, channels, freq_bins, time_frames = x.shape\n",
    "        x = x.view(batch_size, time_frames, -1)  \n",
    "\n",
    "        lengths = lengths // 4\n",
    "        lengths = torch.clamp(lengths, min=1)\n",
    "\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            LSTM_output, LSTM_states = self.rnn(x)\n",
    "            return LSTM_states[0][-1]\n",
    "        else:\n",
    "            RNN_output, RNN_states = self.rnn(x)\n",
    "            return RNN_states[-1]\n",
    "\n",
    "    def forward(self, x: PackedSequence, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.feature_extractor(x, lengths)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x: PackedSequence, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            x = self.forward(x, lengths)\n",
    "            _, predicted = torch.max(x, 1)\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "6451fda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VorgabeLSTM(VorgabeRNN):\n",
    "    def __init__(self, hidden_size=50, num_layers=2, num_classes=NUM_CLASSES, n_fft=512, hop_length=256, dropout_rate=0.2):\n",
    "        super().__init__(hidden_size, num_layers, num_classes, n_fft, hop_length, dropout_rate)\n",
    "        self.rnn = nn.LSTM(self.conv_output_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "class VorgabeGRU(VorgabeRNN):\n",
    "    def __init__(self, hidden_size=50, num_layers=2, num_classes=NUM_CLASSES, n_fft=512, hop_length=256, dropout_rate=0.2):\n",
    "        super().__init__(hidden_size, num_layers, num_classes, n_fft, hop_length, dropout_rate)\n",
    "        self.rnn = nn.GRU(self.conv_output_size, hidden_size, num_layers, batch_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "f0caa10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm\n",
    "class RNNwithSVM(VorgabeRNN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size=50,\n",
    "        num_layers=2,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        n_fft=512,\n",
    "        hop_length=256,\n",
    "        dropout_rate=0.2,\n",
    "    ):\n",
    "        super(RNNwithSVM, self).__init__(\n",
    "            hidden_size, num_layers, num_classes, n_fft, hop_length, dropout_rate\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size) \n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.svm = sklearn.svm.SVC()\n",
    "        self.is_svm_trained = False\n",
    "\n",
    "    def forward(self, x: PackedSequence, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.feature_extractor(x, lengths)[-1]\n",
    "        x = self.fc1(x)\n",
    "        if self.fc2 is not None and not self.is_svm_trained:\n",
    "            return self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x: PackedSequence, lengths: torch.Tensor) -> torch.Tensor:\n",
    "        if not self.is_svm_trained:\n",
    "            raise ValueError(\"SVM not trained yet!\")\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            x = self.feature_extractor(x, lengths)[-1]\n",
    "            x = self.fc1(x)\n",
    "            x = self.svm.predict(x.cpu().numpy())\n",
    "        return x\n",
    "    def _remove_temp_classifier(self):\n",
    "        self.fc2 = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "99f21593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: tensor([64, 64, 64,  ..., 10, 10, 10])\n",
      "Signal lengths: tensor([18000,  9000,  9000,  9000,  9000,  4500,  9000, 14648, 11520,  9000,\n",
      "         8226,  5834,  9000,  9000,  9000,  9000,  9000,  9000,  9000, 18000,\n",
      "         9000,  9000, 18000,  9000,  9000, 10582,  9000,  9000,  9000,  9000,\n",
      "         9000,  9000,  9000, 18000,  9000,  9000,  9000, 18000, 18000,  9000,\n",
      "        18000, 18000,  9000,  9000,  9000,  9000,  9000,  9000,  9000,  9000,\n",
      "         9000,  6162,  9000,  9000,  9000,  9000,  9000,  9000,  9000, 18000,\n",
      "         9000,  9000, 18000,  9000])\n",
      "Signal shapes: [torch.Size([664472, 1]), torch.Size([18000]), torch.Size([64]), torch.Size([64])]\n",
      "actual signal lengths: 18000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = ECGDataset(train_df, augmentation=None)\n",
    "val_dataset = ECGDataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "# check the actual signal lengths from the paddedsequences after   x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "for signals, labels, lengths in val_loader:\n",
    "    print(f\"Batch size: {signals.batch_sizes}\")\n",
    "    print(f\"Signal lengths: {lengths}\")\n",
    "    # signals is a PackedSequence\n",
    "    print(f\"Signal shapes: {[s.shape for s in signals]}\")  # Print shapes of each signal in the batch\n",
    "    x, _ = torch.nn.utils.rnn.pad_packed_sequence(signals, batch_first=True)\n",
    "    print(f\"actual signal lengths: {x.shape[1]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "40b9baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, epochs=10, lr=0.001, batch_size_factor=4):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "    model.to(device)\n",
    "\n",
    "    is_nn_svm = hasattr(model, \"svm\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    original_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array([0, 1, 2, 3]),\n",
    "    y=train_df['label'].values\n",
    ")\n",
    "\n",
    "    # Normalize and scale the weights\n",
    "    scaled_weights = original_weights / original_weights.max()  # normalize to max=1\n",
    "    scaled_weights = 0.5 + (scaled_weights * 0.5)  # shrink range to [0.5, 1.0] for balance\n",
    "\n",
    "    weights_tensor = torch.tensor(scaled_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "\n",
    "    print(\"Training RNN feature extractor...\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for signals, labels, lengths in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            signals, labels, lengths = signals.to(device), labels.to(device), lengths.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(signals, lengths)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        acc = 100.0 * correct / total\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}: Train Loss: {total_loss/len(train_loader):.4f}, Train Acc: {acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "    if is_nn_svm:\n",
    "        model._remove_temp_classifier()\n",
    "        _train_svm_phase(model, train_loader, device, batch_size_factor) \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def _train_svm_phase(model, train_loader, device, batch_size_factor):\n",
    "\n",
    "    print(\"Training SVM...\")\n",
    "    model.eval()\n",
    "    all_features, all_labels = [], []\n",
    "    batch_features, batch_labels = [], []\n",
    "    assert model.fc2 is None, \"Temporary classifier should be removed before SVM training\"\n",
    "\n",
    "    # extract_features_from_loader(model, data_loader, device):\n",
    "    with torch.no_grad():\n",
    "        for signals, labels, lengths in tqdm(train_loader, desc=\"Extracting features\"):\n",
    "            signals, lengths = signals.to(device), lengths.to(device)\n",
    "            features = model(signals, lengths)\n",
    "\n",
    "            batch_features.append(features)\n",
    "            batch_labels.append(labels)\n",
    "\n",
    "            if len(batch_features) >= batch_size_factor:\n",
    "                combined_features = torch.cat(batch_features, dim=0).cpu().numpy()\n",
    "                combined_labels = torch.cat(batch_labels, dim=0).numpy()\n",
    "                all_features.append(combined_features)\n",
    "                all_labels.append(combined_labels)\n",
    "                batch_features, batch_labels = [], []\n",
    "\n",
    "\n",
    "    if batch_features:\n",
    "        combined_features = torch.cat(batch_features, dim=0).cpu().numpy()\n",
    "        combined_labels = torch.cat(batch_labels, dim=0).numpy()\n",
    "        all_features.append(combined_features)\n",
    "        all_labels.append(combined_labels)\n",
    "\n",
    "    final_features = np.concatenate(all_features, axis=0)\n",
    "    final_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    print(f\"Training SVM on {len(final_features)} samples\")\n",
    "    model.svm.fit(final_features, final_labels)\n",
    "    model.is_svm_trained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb95fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, device):\n",
    "    print(\"Evaluating model...\")\n",
    "    model.eval()\n",
    "    all_predictions, all_labels = [], []\n",
    "    is_nn_svm = hasattr(model, \"svm\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for signals, labels, lengths in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            signals, lengths = (\n",
    "                signals.to(device),\n",
    "                lengths.to(device),\n",
    "            )\n",
    "\n",
    "            predictions = model.predict(signals, lengths)\n",
    "            if not is_nn_svm:\n",
    "                predictions = predictions.cpu().numpy()\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    print(f\"Final Validation Accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(all_labels, all_predictions))\n",
    "\n",
    "    # confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    # normalize heatmap colors in each row for class distribution\n",
    "    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(\n",
    "        cm_normalized,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=CLASS_NAMES.values(),\n",
    "        yticklabels=CLASS_NAMES.values(),\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c9c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self : __torch__.___torch_mangle_71.VorgabeRNN,\n",
      "      %x.1 : NamedTuple(data : Tensor, batch_sizes : Tensor, sorted_indices : Tensor?, unsorted_indices : Tensor?),\n",
      "      %lengths.1 : Tensor):\n",
      "  %x.5 : Tensor = prim::CallMethod[name=\"feature_extractor\"](%self, %x.1, %lengths.1) # /var/folders/8y/9y3t89w944n3vzl3tq_mzvs80000gn/T/ipykernel_66268/300297223.py:71:12\n",
      "  %fc : __torch__.torch.nn.modules.linear.___torch_mangle_37.Linear = prim::GetAttr[name=\"fc\"](%self)\n",
      "  %x.9 : Tensor = prim::CallMethod[name=\"forward\"](%fc, %x.5) # /var/folders/8y/9y3t89w944n3vzl3tq_mzvs80000gn/T/ipykernel_66268/300297223.py:72:12\n",
      "  return (%x.9)\n",
      "\n",
      "graph(%self : __torch__.___torch_mangle_72.VorgabeLSTM,\n",
      "      %x.1 : NamedTuple(data : Tensor, batch_sizes : Tensor, sorted_indices : Tensor?, unsorted_indices : Tensor?),\n",
      "      %lengths.1 : Tensor):\n",
      "  %x.5 : Tensor = prim::CallMethod[name=\"feature_extractor\"](%self, %x.1, %lengths.1) # /var/folders/8y/9y3t89w944n3vzl3tq_mzvs80000gn/T/ipykernel_66268/300297223.py:71:12\n",
      "  %fc : __torch__.torch.nn.modules.linear.___torch_mangle_37.Linear = prim::GetAttr[name=\"fc\"](%self)\n",
      "  %x.9 : Tensor = prim::CallMethod[name=\"forward\"](%fc, %x.5) # /var/folders/8y/9y3t89w944n3vzl3tq_mzvs80000gn/T/ipykernel_66268/300297223.py:72:12\n",
      "  return (%x.9)\n",
      "\n",
      "graph(%self : __torch__.___torch_mangle_73.VorgabeGRU,\n",
      "      %x.1 : NamedTuple(data : Tensor, batch_sizes : Tensor, sorted_indices : Tensor?, unsorted_indices : Tensor?),\n",
      "      %lengths.1 : Tensor):\n",
      "  %x.5 : Tensor = prim::CallMethod[name=\"feature_extractor\"](%self, %x.1, %lengths.1) # /var/folders/8y/9y3t89w944n3vzl3tq_mzvs80000gn/T/ipykernel_66268/300297223.py:71:12\n",
      "  %fc : __torch__.torch.nn.modules.linear.___torch_mangle_37.Linear = prim::GetAttr[name=\"fc\"](%self)\n",
      "  %x.9 : Tensor = prim::CallMethod[name=\"forward\"](%fc, %x.5) # /var/folders/8y/9y3t89w944n3vzl3tq_mzvs80000gn/T/ipykernel_66268/300297223.py:72:12\n",
      "  return (%x.9)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_shape = (BATCH_SIZE, INPUT_LENGTH, 1)\n",
    "\n",
    "for model in [VorgabeRNN, VorgabeLSTM, VorgabeGRU]:\n",
    "\n",
    "    model_instance = model(\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        n_fft=256,\n",
    "        hop_length=128,\n",
    "        dropout_rate=0.2,\n",
    "    )\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_instance = model_instance.to(device)\n",
    "    model_instance.eval()\n",
    "    device = \"mps\"\n",
    "    dummy_input = torch.randn(*input_shape, device=device)\n",
    "    dummy_input = pack_sequence(\n",
    "        [dummy_input] * input_shape[0], enforce_sorted=False\n",
    "    )  \n",
    "    dummy_lengths = torch.tensor(\n",
    "        [input_shape[1]] * input_shape[0], dtype=torch.int64, device=device\n",
    "    )\n",
    "    scripted = torch.jit.script(model_instance)\n",
    "    print(scripted.graph)\n",
    "    scripted.save(f\"{model.__name__}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fa00fa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients for rnn.weight_hh_l0 are zero.\n",
      "Gradients for rnn.weight_hh_l1 are zero.\n",
      "Gradients for rnn.weight_hh_l0 are zero.\n",
      "Gradients for rnn.weight_hh_l1 are zero.\n",
      "Gradients for rnn.weight_hh_l0 are zero.\n",
      "Gradients for rnn.weight_hh_l1 are zero.\n"
     ]
    }
   ],
   "source": [
    "def check_gradients(model):\n",
    "    model.train()  \n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    dummy_sequences = []\n",
    "    for i in range(BATCH_SIZE):  \n",
    "        seq = torch.randn(\n",
    "            1000, 1, device=device, requires_grad=True\n",
    "        )\n",
    "        dummy_sequences.append(seq)\n",
    "\n",
    "    dummy_input = pack_sequence(dummy_sequences, enforce_sorted=False)\n",
    "    dummy_lengths = torch.tensor(\n",
    "        [1000] * BATCH_SIZE, dtype=torch.int64, device=device\n",
    "    )\n",
    "\n",
    "    output = model(dummy_input, dummy_lengths)\n",
    "    loss = output.sum()  \n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            if param.grad.abs().sum() == 0:\n",
    "                print(f\"Gradients for {name} are zero.\")\n",
    "\n",
    "\n",
    "for model in [VorgabeRNN, VorgabeLSTM, VorgabeGRU]:\n",
    "    model_instance = model(\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        n_fft=256,\n",
    "        hop_length=128,\n",
    "        dropout_rate=0.2,\n",
    "    )\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_instance = model_instance.to(device)\n",
    "    check_gradients(model_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1133f80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tests with different configurations...\n",
      "Testing VorgabeLSTM with all\n",
      "Training RNN feature extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 83/83 [01:18<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.1139, Train Acc: 58.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 83/83 [01:00<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.0830, Train Acc: 58.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 83/83 [00:57<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 1.0788, Train Acc: 58.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 83/83 [00:57<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 1.0739, Train Acc: 58.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 83/83 [00:43<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 1.0737, Train Acc: 58.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 83/83 [00:41<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 1.0685, Train Acc: 58.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 83/83 [00:44<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 1.0714, Train Acc: 58.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 83/83 [00:44<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 1.0661, Train Acc: 58.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 83/83 [00:50<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 1.0680, Train Acc: 58.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 83/83 [00:47<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 1.0644, Train Acc: 58.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 83/83 [01:02<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 1.0612, Train Acc: 58.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 83/83 [00:50<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 1.0622, Train Acc: 58.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 83/83 [00:42<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 1.0629, Train Acc: 58.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 83/83 [00:46<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 1.0701, Train Acc: 58.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 83/83 [00:52<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 1.0697, Train Acc: 58.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 83/83 [00:50<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss: 1.0778, Train Acc: 58.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:  58%|█████▊    | 48/83 [00:26<00:19,  1.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[254], line 53\u001b[0m\n\u001b[1;32m     42\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     43\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     44\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m model_instance\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate_model(trained_model, val_loader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maugmentation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[251], line 42\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, epochs, lr, batch_size_factor)\u001b[0m\n\u001b[1;32m     40\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(signals, lengths)\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     45\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Uni/AMLS/ecg-timeseries-model/.venv/lib/python3.13/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Uni/AMLS/ecg-timeseries-model/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Uni/AMLS/ecg-timeseries-model/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_space = {\n",
    "    \"model\": [VorgabeLSTM, VorgabeGRU, VorgabeRNN],\n",
    "    \"augmentation\": [\"all\"],\n",
    "}\n",
    "\n",
    "results = []\n",
    "print(\"Starting tests with different configurations...\")\n",
    "\n",
    "for aug in test_space[\"augmentation\"]:\n",
    "    train_set = ECGDataset(train_df, augmentation=aug)\n",
    "    val_set = ECGDataset(val_df)\n",
    "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, generator=torch.Generator().manual_seed(42), num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=collate_fn)\n",
    "\n",
    "    for model_cls in test_space[\"model\"]:\n",
    "        print(f\"Testing {model_cls.__name__} with {aug}\")\n",
    "        model = model_cls(hidden_size=128, \n",
    "                          num_layers=2, \n",
    "                          num_classes=NUM_CLASSES, \n",
    "                          n_fft=N_FFT, \n",
    "                          hop_length=HOP_LENGTH, \n",
    "                          dropout_rate=0.2)\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "        model = model.to(device)\n",
    "        trained = train_model(model, train_loader, epochs=EPOCHS, lr=0.001, batch_size_factor=4)\n",
    "        acc = evaluate_model(trained, val_loader, device)\n",
    "\n",
    "        print(f\"Validation Accuracy of {model_cls.__name__} with {aug}: {acc:.4f}\\n\")\n",
    "        results.append({\"model\": model_cls.__name__, \"augmentation\": aug, \"accuracy\": acc})\n",
    "        del trained\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    del train_set, val_set, train_loader, val_loader\n",
    "\n",
    "print(\"All tests completed.\")\n",
    "best = max(results, key=lambda x: x[\"accuracy\"])\n",
    "print(f\"Best Model: {best['model']}, Augmentation: {best['augmentation']}, Accuracy: {best['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbdbc88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002db5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amls-project-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
